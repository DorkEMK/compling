{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "morph = MorphAnalyzer()\n",
    "stops = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные: датасет НГ  https://github.com/mannefedov/ru_kw_eval_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# скачаем данные в папке data и распакуем их\n",
    "PATH_TO_DATA = './data'\n",
    "files = [os.path.join(PATH_TO_DATA, file) for file in os.listdir(PATH_TO_DATA)]\n",
    "data = pd.concat([pd.read_json(file, lines=True) for file in files][:1], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмём функцию оценивания из опорного ноутбука"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(true_kws, predicted_kws):\n",
    "    assert len(true_kws) == len(predicted_kws)\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    jaccards = []\n",
    "    \n",
    "    for i in range(len(true_kws)):\n",
    "        true_kw = set(true_kws[i])\n",
    "        predicted_kw = set(predicted_kws[i])\n",
    "        \n",
    "        tp = len(true_kw & predicted_kw)\n",
    "        union = len(true_kw | predicted_kw)\n",
    "        fp = len(predicted_kw - true_kw)\n",
    "        fn = len(true_kw - predicted_kw)\n",
    "        \n",
    "        if (tp+fp) == 0:\n",
    "            prec = 0\n",
    "        else:\n",
    "            prec = tp / (tp + fp)\n",
    "        \n",
    "        if (tp+fn) == 0:\n",
    "            rec = 0\n",
    "        else:\n",
    "            rec = tp / (tp + fn)\n",
    "        if (prec+rec) == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = (2*(prec*rec))/(prec+rec)\n",
    "            \n",
    "        jac = tp / union\n",
    "        \n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1s.append(f1)\n",
    "        jaccards.append(jac)\n",
    "    #print('Precision - ', round(np.mean(precisions), 2))\n",
    "    #print('Recall - ', round(np.mean(recalls), 2))\n",
    "    print('F1 - ', round(np.mean(f1s), 2))\n",
    "    #print('Jaccard - ', round(np.mean(jaccards), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как выглядят наборы ключевых слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  [школа, образовательные стандарты, литература, история, фгос]\n",
       "1                                                                              [красота, законы]\n",
       "2                                                [юзефович, гражданская война, пепеляев, якутия]\n",
       "3                                                    [формула1, автоспорт, гонки, испания, квят]\n",
       "4    [есенин, православие, святая русь, поэзия, год литературы, клюев, мариенгоф, стихи, россия]\n",
       "5                             [медвузы, медицинское образование, рудн, николай стуров, интервью]\n",
       "6                  [литература, книги, периодика, космос, небо, астрономия, анатомия, филология]\n",
       "7                                                                             [сша, ирак, война]\n",
       "8                                        [искусственный интеллект, робот, компьютер, технологии]\n",
       "9                                                                  [вб, вто, переговоры, тарифы]\n",
       "Name: keywords, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keywords.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наблюдения:\n",
    "1. Помимо существительных, в ключевых словах встречаются прилагательные\n",
    "2. Среднее количество слов, вероятно, меньше 10\n",
    "3. Сохранено мн.ч. существительного"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим 2-е утверждение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.181174089068826\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for i in data.keywords:\n",
    "    s += len(i)\n",
    "print(s/len(data.keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И посмотрим, что происходит в нормализации с числом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='законы', tag=OpencorporaTag('NOUN,inan,masc plur,accs'), normal_form='закон', score=0.714285, methods_stack=((<DictionaryAnalyzer>, 'законы', 33, 9),)),\n",
       " Parse(word='законы', tag=OpencorporaTag('NOUN,inan,masc plur,nomn'), normal_form='закон', score=0.285714, methods_stack=((<DictionaryAnalyzer>, 'законы', 33, 6),))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph = MorphAnalyzer()\n",
    "morph.parse('законы')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся \"лучшими\" способами оценки из опорного ноутбука, с учётом трёх наблюдений.  \n",
    "Будем применять их по отдельности и в комбинациях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "а) изменим топ частотных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0] for word in words if word and word not in stops]\n",
    "    words = [word.normal_form for word in words if word.tag.POS == 'NOUN']\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_norm'] = data['content'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 -  0.17\n"
     ]
    }
   ],
   "source": [
    "# топ 6 частотных слов статьи\n",
    "evaluate(data['keywords'], data['content_norm'].apply(lambda x: [x[0] for x in Counter(x).most_common(6)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Выбор топ-6 вместо топ-10 частотных слов статьи привел к F1 > бейзлайна**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Изменим только POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_pos(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0] for word in words if word and word not in stops]\n",
    "    words = [word.normal_form for word in words if word.tag.POS in ['NOUN', 'ADJF']]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_norm_pos'] = data['content'].apply(normalize_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 -  0.15\n"
     ]
    }
   ],
   "source": [
    "# топ 10 частотных слов статьи\n",
    "evaluate(data['keywords'], data['content_norm_pos'].apply(lambda x: [x[0] for x in Counter(x).most_common(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Дополнительный выбор прилагательных ухудшил результат по сравнению с бейзлайном*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Добавим прилагательные и выберем топ-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 -  0.15\n"
     ]
    }
   ],
   "source": [
    "# топ 6 частотных слов статьи\n",
    "evaluate(data['keywords'], data['content_norm_pos'].apply(lambda x: [x[0] for x in Counter(x).most_common(6)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*сочетание с выбором топ-6 не помогло*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Приведём слово не в нормальную форму, а в И.п."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_form(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0] for word in words if word and word not in stops]\n",
    "    words = [word.inflect({'nomn'}).word for word in words if word.tag.POS == 'NOUN']\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_norm_form'] = data['content'].apply(normalize_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 -  0.19\n"
     ]
    }
   ],
   "source": [
    "# топ 10 частотных слов статьи\n",
    "evaluate(data['keywords'], data['content_norm_form'].apply(lambda x: [x[0] for x in Counter(x).most_common(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Приведение слов к именительному падежу с сохранением числа привело к F1 > бейзлайна**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. То же самое, но топ-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 -  0.2\n"
     ]
    }
   ],
   "source": [
    "# топ 6 частотных слов статьи\n",
    "evaluate(data['keywords'], data['content_norm_form'].apply(lambda x: [x[0] for x in Counter(x).most_common(6)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Приведение слов к именительному падежу с сохранением числа и выбором топ-6 вместо топ-10 привело к F1 > бейзлайна**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Добавим прилагательные и будем приводить к и.п."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_form_pos(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0] for word in words if word and word not in stops]\n",
    "    words = [word.inflect({'nomn'}).word for word in words if word.tag.POS in ['NOUN', 'ADJF']]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_norm_form_pos'] = data['content'].apply(normalize_form_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 -  0.18\n"
     ]
    }
   ],
   "source": [
    "# топ 10 частотных слов статьи\n",
    "evaluate(data['keywords'], data['content_norm_form_pos'].apply(lambda x: [x[0] for x in Counter(x).most_common(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 -  0.19\n"
     ]
    }
   ],
   "source": [
    "# топ 6 частотных слов статьи\n",
    "evaluate(data['keywords'], data['content_norm_form_pos'].apply(lambda x: [x[0] for x in Counter(x).most_common(6)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Добавление прилагательных ухудшает результат :(**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### То же с tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_norm_str'] = data['content_norm'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5)\n",
    "tfidf.fit(data['content_norm_str'])\n",
    "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}\n",
    "texts_vectors = tfidf.transform(data['content_norm_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сортируем и возьмём топ-6\n",
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-7:-1]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 -  0.17\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Вот и отлично, снова помогло**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 -  0.15\n"
     ]
    }
   ],
   "source": [
    "data['content_norm_pos_str'] = data['content_norm_pos'].apply(' '.join)\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5)\n",
    "tfidf.fit(data['content_norm_pos_str'])\n",
    "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}\n",
    "texts_vectors = tfidf.transform(data['content_norm_pos_str'])\n",
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-11:-1]] \n",
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 -  0.16\n"
     ]
    }
   ],
   "source": [
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-7:-1]] \n",
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Прилагательные делают хуже *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем с приведением к и.п."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 -  0.17\n"
     ]
    }
   ],
   "source": [
    "data['content_norm_form_str'] = data['content_norm_form'].apply(' '.join)\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5)\n",
    "tfidf.fit(data['content_norm_form_str'])\n",
    "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}\n",
    "texts_vectors = tfidf.transform(data['content_norm_form_str'])\n",
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-11:-1]] \n",
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 -  0.19\n"
     ]
    }
   ],
   "source": [
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-7:-1]] \n",
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Стало лучше! **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 -  0.18\n"
     ]
    }
   ],
   "source": [
    "data['content_norm_form_pos_str'] = data['content_norm_form_pos'].apply(' '.join)\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5)\n",
    "tfidf.fit(data['content_norm_form_pos_str'])\n",
    "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}\n",
    "texts_vectors = tfidf.transform(data['content_norm_form_pos_str'])\n",
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-7:-1]] \n",
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*и вновь прилагательные не помогают*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ольга васильев', 'стандарты', 'васильев', 'ольга', 'источник', 'письмо'],\n",
       " ['красота', 'отчаяние', 'кошка', 'глаза', 'скамейка', 'познание'],\n",
       " ['якутия', 'книга', 'восстание', 'сибирь', 'леонид', 'места'],\n",
       " ['гонка', 'команда', 'пилоты', 'сезон', 'при', 'гран при'],\n",
       " ['есенин', 'поэт', 'борода', 'смерть', 'стихи', 'лев']]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "возможно, биграммы-имена собственные вроде \"ольга васильев\" немного засоряют нам результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### попробуем tfidf и топ-6 без биграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 -  0.18\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=5)\n",
    "tfidf.fit(data['content_norm_str'])\n",
    "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}\n",
    "texts_vectors = tfidf.transform(data['content_norm_str'])\n",
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-7:-1]] \n",
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Без учёта биграмм стало лучше**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведение к И.п. без биграмм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 -  0.19\n"
     ]
    }
   ],
   "source": [
    "data['content_norm_form_str'] = data['content_norm_form'].apply(' '.join)\n",
    "tfidf = TfidfVectorizer(min_df=5)\n",
    "tfidf.fit(data['content_norm_form_str'])\n",
    "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}\n",
    "texts_vectors = tfidf.transform(data['content_norm_form_str'])\n",
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-7:-1]] \n",
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Так же, как и с биграммами*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 -  0.18\n"
     ]
    }
   ],
   "source": [
    "data['content_norm_form_pos_str'] = data['content_norm_form_pos'].apply(' '.join)\n",
    "tfidf = TfidfVectorizer(min_df=5)\n",
    "tfidf.fit(data['content_norm_form_pos_str'])\n",
    "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}\n",
    "texts_vectors = tfidf.transform(data['content_norm_form_pos_str'])\n",
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-7:-1]] \n",
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ещё один повод убедиться, что прилагательные не помогают*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итог:\n",
    "- сокращение топа частотных слов с 10 до 6 приводит к превышению бейзлайна во всех трёх случаях\n",
    "- приводить слово не к нормальной форме, а к Им.п. приводит к превышению бейзлайна во всех трёх случаях\n",
    "- добавление прилагательных не делает результат лучше, а иногда ухудшает\n",
    "- применение tfidf в сочетании с описанными приёмами позволяет побить бейзлайн, но не приводит к улучшению F1 по сранению с использованием топа частотных слов\n",
    "- рассмотрение только униграмм улучшает результат только для топ-6 без изменения в нормализации\n",
    "\n",
    "- лучший результат: F1 = 0.2 получен при выборе 6 самых частотных слов из статьи с приведением слов к форме Им.п. вместо нормальной формы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAKE\n",
    "Попробуем готовую имплементацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Metric, Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Rake(language='russian', stopwords=stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = pd.Series.tolist(data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.extract_keywords_from_text(cont[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['обозревателем « нг » ольга васильева прокомментировала претензии гильдии словесников',\n",
       " 'каждому предмету разработано 15 – 20 конкретных требований',\n",
       " 'министерство образования дает право контролирующим органам ловить детей',\n",
       " '« сформированность умений проводить атрибуцию текстового исторического источника',\n",
       " 'прежних стандартах результатом изучения курса истории должно',\n",
       " 'каждого года обучения приведены предметные результаты обучения',\n",
       " '– заявила « нг » ольга васильева',\n",
       " 'накануне гильдия словесников разместила открытое письмо',\n",
       " 'новые фгосы грубо нарушают права детей',\n",
       " 'федеральных государственных образовательных стандартах начального общего']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.get_ranked_phrases()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, можно даже не считать F1  \n",
    "Что если уменьшить длину фраз?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Rake(language='russian', stopwords=stopwords.words('russian'), min_length=1, max_length=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = pd.Series.tolist(data['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "посмотрим на одном тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.extract_keywords_from_text(cont[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['число книг',\n",
       " 'цели создания',\n",
       " 'участников событий',\n",
       " 'увязываю это',\n",
       " 'точки зрения',\n",
       " 'существующему стандарту',\n",
       " 'станет понятно',\n",
       " 'средней школы',\n",
       " 'сохранена структура',\n",
       " 'сонме романов']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.get_ranked_phrases()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "не обнадёживает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Rake(language='russian', stopwords=stopwords.words('russian'), min_length=1, max_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = pd.Series.tolist(data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.extract_keywords_from_text(cont[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['–',\n",
       " 'явлениях',\n",
       " 'явления',\n",
       " 'явлений',\n",
       " 'читают',\n",
       " 'читали',\n",
       " 'читала',\n",
       " 'числе',\n",
       " 'фгосам',\n",
       " 'фгос']"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.get_ranked_phrases()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итог\n",
    "Rake \"из коробки\" не подходит. Надо что-то делать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
